{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://static1.squarespace.com/static/5ea57eb1c6398826cb20f779/t/5ea5875b36c5d45a8f64f9f3/1595005115947/?format=180w 'DNAstack logo')\n",
    "\n",
    "\n",
    "_For demonstration purposes only_\n",
    "\n",
    "# Machine Learning Using COVID Cloud and Azure\n",
    "\n",
    "In this notebook we will be demonstrating how to do machine learning with Azure Notebooks using SARS-CoV-2 variant data from COVID Cloud.\n",
    "\n",
    "The machine learning in this notebook will include:\n",
    "\n",
    "1. Linear Regression using `statsmodels`\n",
    "2. K-neighest-neighbor using `scikit-learn`\n",
    "3. Deep Learning Compression using TensorFlow / Keras\n",
    "\n",
    "Before you begin, the first thing that is needed to be done is to download the libraries required for this analysis, \n",
    "and restart the notebook if necessary. In order to download the necessary libraries, make sure the `requirements.txt`\n",
    "file is included in the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "pip install --no-cache-dir -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next and most important step is to use the search library to download COVID variant data. \n",
    "\n",
    "There are 2 main tables we want to use for the analysis:\n",
    "- Variant table\n",
    "- Meta table \n",
    "\n",
    "The following downloads should be complete in less than a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from dnastack import PublisherClient\n",
    "\n",
    "#### Create the client\n",
    "dataconnect_url = 'https://collection-service.publisher.dnastack.com/collection/library/search/'\n",
    "publisher_client = PublisherClient(dataconnect_url=dataconnect_url)\n",
    "\n",
    "#### Download metadata\n",
    "print('Fetching metadata from DNAstack…')\n",
    "query = \"SELECT * FROM covid.cloud.sequences WHERE collection_date < date('2020-07-01')\"\n",
    "meta_df = pd.DataFrame(publisher_client.dataconnect.query(query))\n",
    "print(\"Metadata:\")\n",
    "print(meta_df)\n",
    "\n",
    "#### Download variant data\n",
    "print('Fetching variants from DNAstack…')\n",
    "query = \"\"\"\n",
    "SELECT v.*\n",
    "FROM covid.cloud.variants AS v JOIN covid.cloud.sequences AS s ON s.accession = sequence_accession\n",
    "WHERE s.collection_date < date('2020-07-01')\n",
    "\"\"\"\n",
    "variant_df = pd.DataFrame(publisher_client.dataconnect.query(query))\n",
    "print(\"Variant Data:\")\n",
    "print(variant_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Regression using Statsmodels\n",
    "In this example, you will be using the `statsmodels` library to perform linear regression to \n",
    "find out what the rate of change of a particular variant is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "##### Release date --> Day of the year\n",
    "meta_df.release_date = meta_df.release_date.astype('datetime64').apply(lambda x: (x - timedelta(days=x.dayofweek)))\n",
    "\n",
    "##### Get positions for SNP of interest\n",
    "accessions = variant_df[variant_df.start_position == 23402].sequence_accession\n",
    "pos_count = meta_df[(meta_df.accession.isin(accessions))].groupby('release_date').count().host\n",
    "neg_count = meta_df[~(meta_df.accession.isin(accessions))].groupby('release_date').count().host\n",
    "\n",
    "##### Merge dataframes\n",
    "merge_df = pd.merge(neg_count, pos_count, left_index=True, right_index=True, how='inner').fillna(0)\n",
    "\n",
    "##### Linear regression\n",
    "y = merge_df.host_y / merge_df.sum(axis=1)\n",
    "x = sm.add_constant(np.arange(len(y)))\n",
    "model = sm.OLS(y, x).fit()\n",
    "b1 = model.params[1].round(3)\n",
    "p = model.pvalues[1].round(3)\n",
    "print(f'The weekly rate of change is {b1} with a p-value of {p}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. K-nearest-neighbor using Scikit-learn\n",
    "\n",
    "In this example, scikit-learn is used to perform clustering with the K-nearest-neighbor algorithm. The goal of this analysis is to see whether there is a difference in the mutation profiles of American and Australian samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "##### Get the meta dataframe of USA, Australia\n",
    "meta_df_oi = meta_df[(meta_df.location == 'USA') |\n",
    "                     (meta_df.location == 'Australia: Victoria')]\n",
    "\n",
    "##### Filter variant_df by meta_df accessions\n",
    "v_df = variant_df[variant_df.sequence_accession.isin(meta_df_oi.accession)]\n",
    "\n",
    "##### Get the top 100 most prevalent mutations\n",
    "v_df = v_df[v_df.start_position.isin(v_df.start_position.value_counts().head(100).index)]\n",
    "\n",
    "##### Crosstab\n",
    "ct = pd.crosstab(v_df.sequence_accession, v_df.start_position)\n",
    "\n",
    "##### Get locations (y-variable)\n",
    "locations = pd.merge(ct,\n",
    "                     meta_df_oi[['accession', 'location']],\n",
    "                     left_index=True,\n",
    "                     right_on='accession').location\n",
    "\n",
    "##### Cross tabbed data to pca\n",
    "X_pca = PCA(n_components=2).fit_transform(ct.values)\n",
    "\n",
    "##### KNN classification\n",
    "knn = KNeighborsClassifier().fit(X_pca, locations)\n",
    "\n",
    "##### Predictions\n",
    "predictions = knn.predict(X_pca)\n",
    "\n",
    "##### Figure\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "##### Plot\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=(predictions == 'USA').astype(int))\n",
    "\n",
    "##### Labels\n",
    "plt.xlabel('PCA_1')\n",
    "plt.ylabel('PCA_2')\n",
    "\n",
    "##### Legend\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=['Australia', 'USA'])\n",
    "\n",
    "##### Accuracy\n",
    "print('KNN model with training accuracy of :', knn.score(X_pca, locations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deep Learning Compression using TensorFlow / Keras\n",
    "In this example you will be using TensorFlow to compress variant data in order to capture meaningful insights while reducing the size of the variant matrix.\n",
    "\n",
    "Because `tensorflow` is a massive library, its installation is placed seperatly here. Run the cell below to install `tensorflow`. The entire download should take <5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pip install --no-cache-dir tensorflow==2.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "##### Get the meta dataframe of USA, Australia\n",
    "meta_df_oi = meta_df[(meta_df.location == 'USA') |\n",
    "                     (meta_df.location == 'Australia: Victoria')]\n",
    "\n",
    "##### Filter variant_df by meta_df accessions\n",
    "v_df = variant_df[variant_df.sequence_accession.isin(meta_df_oi.accession)]\n",
    "\n",
    "##### Get the top 100 most prevalent mutations\n",
    "v_df = v_df[v_df.start_position.isin(v_df.start_position.value_counts().head(100).index)]\n",
    "\n",
    "##### Crosstab\n",
    "ct = pd.crosstab(v_df.sequence_accession, v_df.start_position)\n",
    "\n",
    "##### Get locations (y-variable)\n",
    "locations = pd.merge(ct,\n",
    "                     meta_df_oi[['accession', 'location']],\n",
    "                     left_index=True,\n",
    "                     right_on='accession').location\n",
    "\n",
    "##### Cross tabbed data to pca\n",
    "X_pca = PCA(n_components=2).fit_transform(ct.values)\n",
    "\n",
    "##### KNN classification\n",
    "knn = KNeighborsClassifier().fit(X_pca, locations)\n",
    "\n",
    "##### Predictions\n",
    "predictions = knn.predict(X_pca)\n",
    "\n",
    "##### Figure\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "##### Plot\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=(predictions == 'USA').astype(int))\n",
    "\n",
    "##### Labels\n",
    "plt.xlabel('PCA_1')\n",
    "plt.ylabel('PCA_2')\n",
    "\n",
    "##### Legend\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=['Australia', 'USA'])\n",
    "\n",
    "##### Accuracy\n",
    "print('KNN model with training accuracy of :', knn.score(X_pca, locations))"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
